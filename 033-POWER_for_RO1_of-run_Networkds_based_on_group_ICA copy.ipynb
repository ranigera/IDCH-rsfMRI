{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "* See notes form file 02 (run_group_ICA) (just look at the most updated version of it). Some notes are VERY relevant.\n",
    "\n",
    "* Look at NEXT: note at the bottom of this page.\n",
    "\n",
    "* pip install git+https://git.fmrib.ox.ac.uk/fsl/fslnets.git\n",
    "Needed to be run to use this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff + params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl import nets\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# need to set up to run fsl.\n",
    "os.environ['FSLDIR'] = '/export/home/ranigera/fsl'\n",
    "os.environ['PATH'] += ':/export/home/ranigera/fsl/bin'\n",
    "\n",
    "group_ICA_path = '/export/home/ranigera/IDCH-rsfMRI/data/group_ICA'\n",
    "\n",
    "whole_network_path = '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks'\n",
    "\n",
    "# n_ICs = 50 # number of independent components\n",
    "\n",
    "# R01 to IDCH mapping:\n",
    "mapping_R01_to_IDCH = {'222': '101', '183': '102', '216': '103', '192': '104', '251': '105', '206': '106', '180': '107', '184': '108', '169': '109', '207': '110',\n",
    "                       '159': '111', '115': '112', '114': '113', '232': '114', '173': '115', '171': '117', '215': '118', '265': '119', '177': '120', '269': '121',\n",
    "                       '261': '122'}\n",
    "\n",
    "main_behav_file = '/export/home/ranigera/IDCH-rsfMRI/data/behav_data/merged_behav_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ICs=50 # number of independent components\n",
    "# run slices_summary:\n",
    "# check if exists:\n",
    "if not os.path.exists(f'{group_ICA_path}/groupICA{n_ICs}.sum'):\n",
    "    os.system(f'slices_summary {group_ICA_path}/groupICA{n_ICs}/melodic_IC 4 $FSLDIR/data/standard/MNI152_T1_2mm {group_ICA_path}/groupICA{n_ICs}.sum -1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the time series to a dedicated folder and assign the right subject (IDCH) number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ICs=50 # number of independent components\n",
    "\n",
    "by_IDCH_sub_ID_ts_path = os.path.join(whole_network_path, 'IC_ts_by_IDCH_sub_ID')\n",
    "if not os.path.exists(by_IDCH_sub_ID_ts_path):\n",
    "    os.makedirs(by_IDCH_sub_ID_ts_path)\n",
    "\n",
    "dual_regressed_path = os.path.join(f'{group_ICA_path}/groupICA{n_ICs}.dr')\n",
    "file_list = sorted(glob.glob(f'{dual_regressed_path}/*stage1*.txt'))\n",
    "\n",
    "matching_IDCH_sub_list = sorted(mapping_R01_to_IDCH.values())\n",
    "\n",
    "for i in range(0, len(file_list)):\n",
    "    os.system(f'cp {file_list[i]} {by_IDCH_sub_ID_ts_path}/dr_stage1_{matching_IDCH_sub_list[i]}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze corr with behavior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the behavior data:\n",
    "main_behav_data = pd.read_csv(main_behav_file)\n",
    "# change subID to int:\n",
    "main_behav_data['subID'] = main_behav_data['subID'].astype(int)\n",
    "main_behav_data\n",
    "\n",
    "# behav_vars_if_interest = ['SRM_score', 'std_diary', 'normed_std_diary', 'routine_mean', 'meanVal_relativeDiff_deval_SQRT', 'devaluation', 'at_least_one_response_test', 'at_least_one_response_combined', 'mood', 'Anxiety', 'Stress']\n",
    "behav_vars_if_interest = ['SRM_score', 'std_diary', 'routine_mean', 'meanVal_relativeDiff_deval_SQRT', 'devaluation', 'at_least_one_response_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run network-behavior univariate correlation analysis (each node is tested separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_univariate_corr_w_behav_analysis(behav_var, n_ICs=50):\n",
    "    # Initial general stuff:\n",
    "    univariate_behav_corr_path = os.path.join(whole_network_path, f'univariateICA{n_ICs}.behavior_corr')\n",
    "    if not os.path.exists(univariate_behav_corr_path):\n",
    "        os.makedirs(univariate_behav_corr_path)\n",
    "\n",
    "    univariate_behav_corr_input_data_folder = os.path.join(univariate_behav_corr_path, 'input_data')\n",
    "    if not os.path.exists(univariate_behav_corr_input_data_folder):\n",
    "        os.makedirs(univariate_behav_corr_input_data_folder)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Prepare design matrix\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # get the data with subID and the behavior variable (and remove NaNs):\n",
    "    behav_data = main_behav_data[['subID', behav_var]].dropna().reset_index(drop=True)\n",
    "    num_data_points = len(behav_data['subID'])\n",
    "    demeaned_behav_data = behav_data[behav_var] - behav_data[behav_var].mean()\n",
    "\n",
    "    # Prepare the .mat file content\n",
    "    # --------------------------------------------------------------\n",
    "    header = f\"\"\"/NumWaves\\t1\n",
    "    /NumPoints\\t{num_data_points}\n",
    "\n",
    "    /Matrix\n",
    "    \"\"\"\n",
    "    # Format the matrix content\n",
    "    matrix_content = \"\\n\".join([f\"{value}\\t\" for value in demeaned_behav_data])\n",
    "    # Combine header and matrix content\n",
    "    design_matrix_content = header + matrix_content\n",
    "    # print(design_matrix_content)\n",
    "    # Write the design matrix to a file\n",
    "    output_file = f'design_behav_corr_{behav_var}.mat'\n",
    "    output_file = os.path.join(univariate_behav_corr_input_data_folder, output_file)\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(design_matrix_content)\n",
    "    print(f\"Design matrix file '{output_file}' created successfully!\")\n",
    "\n",
    "\n",
    "    # Prepare the .con file content\n",
    "    # --------------------------------------------------------------\n",
    "    # Parameters for the contrast file\n",
    "    num_waves = 1  # As per your new design, only one EV\n",
    "    num_contrasts = 1  # Two contrasts: positive and negative\n",
    "\n",
    "    # Matrix for contrasts\n",
    "    # Positive correlation: 1 for the EV\n",
    "    # Negative correlation: -1 for the EV\n",
    "    contrast_matrix = [\n",
    "        [1.0],  # Positive correlation\n",
    "    ]\n",
    "\n",
    "    # Create the .con file content\n",
    "    header = f\"\"\"/NumWaves\\t{num_waves}\n",
    "    /NumContrasts\\t{num_contrasts}\n",
    "\n",
    "    /Matrix\n",
    "    \"\"\"\n",
    "    # Format the matrix content\n",
    "    matrix_content = \"\\n\".join([\"\\t\".join([f\"{value}\" for value in row]) for row in contrast_matrix])\n",
    "    # Combine header and matrix content\n",
    "    contrast_matrix_content = header + matrix_content\n",
    "    # print(contrast_matrix_content)\n",
    "    # Write the contrast matrix to a file\n",
    "    output_file = f'design_behav_corr_{behav_var}_pos_corr.con'\n",
    "    output_file = os.path.join(univariate_behav_corr_input_data_folder, output_file)\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(contrast_matrix_content)\n",
    "    print(f\"Contrast matrix file '{output_file}' created successfully!\")\n",
    "\n",
    "    # Create the .con file content for negative correlation [HAS TO BE DONE SEPERATELY BECAUSE OF HOW THE nets.glm HANDLE THE CONTRASTS WITH ONE EV]\n",
    "    contrast_matrix = [\n",
    "        [-1.0]  # Negative correlation\n",
    "    ]\n",
    "    # Format the matrix content\n",
    "    matrix_content = \"\\n\".join([\"\\t\".join([f\"{value}\" for value in row]) for row in contrast_matrix])\n",
    "    # Combine header and matrix content\n",
    "    contrast_matrix_content = header + matrix_content\n",
    "    # print(contrast_matrix_content)\n",
    "    # Write the contrast matrix to a file\n",
    "    output_file = f'design_behav_corr_{behav_var}_neg_corr.con'\n",
    "    output_file = os.path.join(univariate_behav_corr_input_data_folder, output_file)\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(contrast_matrix_content)\n",
    "    print(f\"Contrast matrix file '{output_file}' created successfully!\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    ## Load as the relevant data as a ts object\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    relevant_file = [f'{by_IDCH_sub_ID_ts_path}/dr_stage1_{subj}.txt' for subj in behav_data.subID]\n",
    "    ts = nets.load(relevant_file, 0.70, varnorm=0, nruns=1, thumbnaildir=f'{group_ICA_path}/groupICA{n_ICs}.sum')\n",
    "    Fnetmats = nets.netmats(ts, 'corr',   True)\n",
    "    Pnetmats = nets.netmats(ts, 'ridgep', True, 0.1) # partial correlations with regularization\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Test Correlation between behavior and the edges (correlation between nodes)\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    print(f'\\n---------------------------------   Positive correlation for {behav_var} [with partial correlation]:   ---------------------------------')\n",
    "    p_corr,p_uncorr = nets.glm(ts, Pnetmats, os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}.mat'), os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}_pos_corr.con'))\n",
    "    plt.show()\n",
    "    print(f'---------------------------------   Negative correlation for {behav_var} [with partial correlation]:   ---------------------------------')\n",
    "    p_corr,p_uncorr = nets.glm(ts, Pnetmats, os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}.mat'), os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}_neg_corr.con'))\n",
    "    plt.show()\n",
    "    print(f'---------------------------------   Positive correlation for {behav_var} [with full correlation]:   ---------------------------------')\n",
    "    p_corr,p_uncorr = nets.glm(ts, Fnetmats, os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}.mat'), os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}_pos_corr.con'))\n",
    "    plt.show()\n",
    "    print(f'---------------------------------   Negative correlation for {behav_var} [with full correlation]:   ---------------------------------')\n",
    "    p_corr,p_uncorr = nets.glm(ts, Fnetmats, os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}.mat'), os.path.join(univariate_behav_corr_input_data_folder, f'design_behav_corr_{behav_var}_neg_corr.con'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying based on the whole network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subID</th>\n",
       "      <th>sequenceCompleted_test</th>\n",
       "      <th>sequenceCompleted_reacquisition</th>\n",
       "      <th>sequenceCompleted_combined</th>\n",
       "      <th>at_least_one_response_test</th>\n",
       "      <th>at_least_one_response_reacquisition</th>\n",
       "      <th>at_least_one_response_combined</th>\n",
       "      <th>devaluation</th>\n",
       "      <th>stillVal_minus_deval</th>\n",
       "      <th>preVal_relativeDiff_deval</th>\n",
       "      <th>...</th>\n",
       "      <th>std_diary</th>\n",
       "      <th>normed_std_diary</th>\n",
       "      <th>SRM_score</th>\n",
       "      <th>mood</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Stress</th>\n",
       "      <th>routine_mean</th>\n",
       "      <th>any_test_DTH_slips</th>\n",
       "      <th>any_combined_DTH_slips</th>\n",
       "      <th>any_App_slips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281.631619</td>\n",
       "      <td>0.530220</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>71.985294</td>\n",
       "      <td>34.367647</td>\n",
       "      <td>26.191176</td>\n",
       "      <td>3.593137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>966.548720</td>\n",
       "      <td>0.433184</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>68.357143</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.828571</td>\n",
       "      <td>3.638095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>761.555304</td>\n",
       "      <td>0.372977</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>63.100000</td>\n",
       "      <td>20.471429</td>\n",
       "      <td>18.957143</td>\n",
       "      <td>3.823810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1200.262012</td>\n",
       "      <td>0.519515</td>\n",
       "      <td>3.058824</td>\n",
       "      <td>68.457143</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>14.814286</td>\n",
       "      <td>4.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1444.988375</td>\n",
       "      <td>0.595996</td>\n",
       "      <td>1.882353</td>\n",
       "      <td>62.536232</td>\n",
       "      <td>33.927536</td>\n",
       "      <td>42.318841</td>\n",
       "      <td>4.376812</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.724138</td>\n",
       "      <td>3.896552</td>\n",
       "      <td>9.327586</td>\n",
       "      <td>5.902299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1376.074428</td>\n",
       "      <td>0.555342</td>\n",
       "      <td>1.954545</td>\n",
       "      <td>66.402985</td>\n",
       "      <td>42.507463</td>\n",
       "      <td>37.104478</td>\n",
       "      <td>4.457711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1065.073426</td>\n",
       "      <td>0.452931</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>70.609375</td>\n",
       "      <td>25.718750</td>\n",
       "      <td>15.906250</td>\n",
       "      <td>5.072917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>...</td>\n",
       "      <td>971.316793</td>\n",
       "      <td>0.462965</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>86.630769</td>\n",
       "      <td>3.384615</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>5.907692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1014.818277</td>\n",
       "      <td>0.480114</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>66.428571</td>\n",
       "      <td>12.442857</td>\n",
       "      <td>28.271429</td>\n",
       "      <td>5.219048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.914286</td>\n",
       "      <td>11.314286</td>\n",
       "      <td>11.242857</td>\n",
       "      <td>3.942857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>...</td>\n",
       "      <td>1109.391470</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>54.968254</td>\n",
       "      <td>44.492063</td>\n",
       "      <td>47.174603</td>\n",
       "      <td>4.952381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>893.633254</td>\n",
       "      <td>0.433734</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>79.742857</td>\n",
       "      <td>20.028571</td>\n",
       "      <td>20.685714</td>\n",
       "      <td>4.871429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>114</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>...</td>\n",
       "      <td>1324.784719</td>\n",
       "      <td>0.579484</td>\n",
       "      <td>2.592593</td>\n",
       "      <td>63.014493</td>\n",
       "      <td>39.130435</td>\n",
       "      <td>41.478261</td>\n",
       "      <td>4.285024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>1352.127892</td>\n",
       "      <td>0.574931</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>53.769231</td>\n",
       "      <td>38.569231</td>\n",
       "      <td>38.092308</td>\n",
       "      <td>4.246154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1195.987720</td>\n",
       "      <td>0.484335</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>59.030303</td>\n",
       "      <td>25.575758</td>\n",
       "      <td>33.530303</td>\n",
       "      <td>4.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>...</td>\n",
       "      <td>796.090639</td>\n",
       "      <td>0.387920</td>\n",
       "      <td>8.307692</td>\n",
       "      <td>91.842857</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>119</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>750.011386</td>\n",
       "      <td>0.364372</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>33.396825</td>\n",
       "      <td>32.761905</td>\n",
       "      <td>33.619048</td>\n",
       "      <td>3.703704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>-24</td>\n",
       "      <td>-0.324324</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.412698</td>\n",
       "      <td>33.841270</td>\n",
       "      <td>36.301587</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>...</td>\n",
       "      <td>1091.263557</td>\n",
       "      <td>0.494239</td>\n",
       "      <td>2.210526</td>\n",
       "      <td>59.057971</td>\n",
       "      <td>38.956522</td>\n",
       "      <td>38.260870</td>\n",
       "      <td>4.227053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1440.679724</td>\n",
       "      <td>0.536599</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>58.420290</td>\n",
       "      <td>51.739130</td>\n",
       "      <td>53.304348</td>\n",
       "      <td>4.710145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subID  sequenceCompleted_test  sequenceCompleted_reacquisition  \\\n",
       "0     101                       1                               11   \n",
       "1     102                       0                                0   \n",
       "2     103                       0                                0   \n",
       "3     104                       0                                0   \n",
       "4     105                       3                                0   \n",
       "5     106                       1                                0   \n",
       "6     107                       0                                0   \n",
       "7     108                       0                                0   \n",
       "8     109                       0                                0   \n",
       "9     110                       0                                0   \n",
       "10    111                       0                                0   \n",
       "11    112                       0                                0   \n",
       "12    113                       0                                0   \n",
       "13    114                       7                                2   \n",
       "14    115                       0                               10   \n",
       "15    117                       0                               10   \n",
       "16    118                       1                                5   \n",
       "17    119                      11                               11   \n",
       "18    120                       5                                0   \n",
       "19    121                       0                                0   \n",
       "20    122                       0                                0   \n",
       "\n",
       "    sequenceCompleted_combined  at_least_one_response_test  \\\n",
       "0                           12                           1   \n",
       "1                            0                           0   \n",
       "2                            0                           0   \n",
       "3                            0                           0   \n",
       "4                            3                           4   \n",
       "5                            1                           2   \n",
       "6                            0                           0   \n",
       "7                            0                           0   \n",
       "8                            0                           0   \n",
       "9                            0                           1   \n",
       "10                           0                           0   \n",
       "11                           0                           0   \n",
       "12                           0                           0   \n",
       "13                           9                           8   \n",
       "14                          10                           1   \n",
       "15                          10                           0   \n",
       "16                           6                           2   \n",
       "17                          22                          11   \n",
       "18                           5                           8   \n",
       "19                           0                           0   \n",
       "20                           0                           0   \n",
       "\n",
       "    at_least_one_response_reacquisition  at_least_one_response_combined  \\\n",
       "0                                    11                              12   \n",
       "1                                     0                               0   \n",
       "2                                     0                               0   \n",
       "3                                     0                               0   \n",
       "4                                     0                               4   \n",
       "5                                     0                               2   \n",
       "6                                     0                               0   \n",
       "7                                     0                               0   \n",
       "8                                     0                               0   \n",
       "9                                     0                               1   \n",
       "10                                    2                               2   \n",
       "11                                    0                               0   \n",
       "12                                    0                               0   \n",
       "13                                    4                              12   \n",
       "14                                   11                              12   \n",
       "15                                   10                              10   \n",
       "16                                    6                               8   \n",
       "17                                   11                              22   \n",
       "18                                    0                               8   \n",
       "19                                    0                               0   \n",
       "20                                    0                               0   \n",
       "\n",
       "    devaluation  stillVal_minus_deval  preVal_relativeDiff_deval  ...  \\\n",
       "0             0                    34                   1.000000  ...   \n",
       "1             0                     5                   1.000000  ...   \n",
       "2             6                     1                   0.076923  ...   \n",
       "3             3                    12                   0.666667  ...   \n",
       "4             0                     9                   1.000000  ...   \n",
       "5             4                    26                   0.764706  ...   \n",
       "6             0                    10                   1.000000  ...   \n",
       "7             0                     3                   1.000000  ...   \n",
       "8             2                    25                   0.862069  ...   \n",
       "9             7                     7                   0.333333  ...   \n",
       "10            1                     2                   0.500000  ...   \n",
       "11            1                     5                   0.714286  ...   \n",
       "12           44                    22                   0.200000  ...   \n",
       "13            3                    28                   0.823529  ...   \n",
       "14            3                    39                   0.866667  ...   \n",
       "15            0                     6                   1.000000  ...   \n",
       "16           15                    35                   0.538462  ...   \n",
       "17           13                     0                   0.000000  ...   \n",
       "18           49                   -24                  -0.324324  ...   \n",
       "19            2                    15                   0.789474  ...   \n",
       "20           10                    10                   0.333333  ...   \n",
       "\n",
       "      std_diary  normed_std_diary  SRM_score       mood    Anxiety     Stress  \\\n",
       "0   1281.631619          0.530220   2.200000  71.985294  34.367647  26.191176   \n",
       "1    966.548720          0.433184   4.125000  68.357143  22.000000  21.828571   \n",
       "2    761.555304          0.372977   6.000000  63.100000  20.471429  18.957143   \n",
       "3   1200.262012          0.519515   3.058824  68.457143  14.714286  14.814286   \n",
       "4   1444.988375          0.595996   1.882353  62.536232  33.927536  42.318841   \n",
       "5           NaN               NaN        NaN  90.724138   3.896552   9.327586   \n",
       "6   1376.074428          0.555342   1.954545  66.402985  42.507463  37.104478   \n",
       "7   1065.073426          0.452931   1.571429  70.609375  25.718750  15.906250   \n",
       "8    971.316793          0.462965   3.000000  86.630769   3.384615   5.400000   \n",
       "9   1014.818277          0.480114   4.600000  66.428571  12.442857  28.271429   \n",
       "10          NaN               NaN        NaN  82.914286  11.314286  11.242857   \n",
       "11  1109.391470          0.486811   4.300000  54.968254  44.492063  47.174603   \n",
       "12   893.633254          0.433734   5.533333  79.742857  20.028571  20.685714   \n",
       "13  1324.784719          0.579484   2.592593  63.014493  39.130435  41.478261   \n",
       "14  1352.127892          0.574931   3.800000  53.769231  38.569231  38.092308   \n",
       "15  1195.987720          0.484335   3.000000  59.030303  25.575758  33.530303   \n",
       "16   796.090639          0.387920   8.307692  91.842857   1.871429   2.600000   \n",
       "17   750.011386          0.364372   4.333333  33.396825  32.761905  33.619048   \n",
       "18          NaN               NaN        NaN  70.412698  33.841270  36.301587   \n",
       "19  1091.263557          0.494239   2.210526  59.057971  38.956522  38.260870   \n",
       "20  1440.679724          0.536599   2.000000  58.420290  51.739130  53.304348   \n",
       "\n",
       "    routine_mean  any_test_DTH_slips  any_combined_DTH_slips  any_App_slips  \n",
       "0       3.593137                 1.0                     1.0            0.0  \n",
       "1       3.638095                 0.0                     0.0            0.0  \n",
       "2       3.823810                 0.0                     0.0            1.0  \n",
       "3       4.095238                 0.0                     0.0            1.0  \n",
       "4       4.376812                 1.0                     1.0            0.0  \n",
       "5       5.902299                 1.0                     1.0            1.0  \n",
       "6       4.457711                 0.0                     0.0            0.0  \n",
       "7       5.072917                 0.0                     0.0            0.0  \n",
       "8       5.907692                 0.0                     0.0            1.0  \n",
       "9       5.219048                 1.0                     1.0            1.0  \n",
       "10      3.942857                 0.0                     1.0            1.0  \n",
       "11      4.952381                 0.0                     0.0            1.0  \n",
       "12      4.871429                 0.0                     0.0            1.0  \n",
       "13      4.285024                 1.0                     1.0            1.0  \n",
       "14      4.246154                 1.0                     1.0            1.0  \n",
       "15      4.363636                 0.0                     1.0            0.0  \n",
       "16      5.700000                 1.0                     1.0            1.0  \n",
       "17      3.703704                 1.0                     1.0            1.0  \n",
       "18      4.857143                 1.0                     1.0            1.0  \n",
       "19      4.227053                 0.0                     0.0            1.0  \n",
       "20      4.710145                 0.0                     0.0            1.0  \n",
       "\n",
       "[21 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the behavior data:\n",
    "main_behav_data = pd.read_csv(main_behav_file)\n",
    "# change subID to int:\n",
    "main_behav_data['subID'] = main_behav_data['subID'].astype(int)\n",
    "main_behav_data['any_test_DTH_slips'] = (main_behav_data['at_least_one_response_test']>0).astype(float)\n",
    "main_behav_data['any_combined_DTH_slips'] = (main_behav_data['at_least_one_response_combined']>0).astype(float)\n",
    "# now binarize for meanVal_relativeDiff_deval_SQRT:\n",
    "main_behav_data['any_App_slips'] = (main_behav_data['meanVal_relativeDiff_deval_SQRT']<1).astype(float)\n",
    "behav_vars_if_interest = ['any_test_DTH_slips', 'any_combined_DTH_slips', 'any_App_slips']\n",
    "main_behav_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE!\n",
    "Below is the FSL function for classify (the nets.classify)\n",
    "1) The last 4 lines are actually wrong in  my (and chatGPT's) understanding because it tries to predict all the data based on the model from the last iteration of the leave one out cross validation whichi does not make sense.\n",
    "2) In any case, the important score is the \"Accuracy during training:...\" part. This is the crossvalidation result.\n",
    "3) **>>Cacelled due to deciding to standardize and concatenate<<** I created the classify_when_multiple_sub_session to not include any subject data in the training when it is the \"one out\" in the cross validation. So effectively it is like leave two out (by the same subject) for each round\n",
    "4) And I also created my own version function of classify (doing the same thing, but I added some stuff for convenience and removed these 4 last lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#\n",
    "# classify.py - Train a classifier on netmats to diffentiate groups.\n",
    "\n",
    "# Author: Paul McCarthy <pauldmccarthy@gmail.com>\n",
    "#\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy                          as     np\n",
    "from   sklearn.pipeline               import Pipeline\n",
    "from   sklearn.preprocessing          import StandardScaler\n",
    "from   sklearn.discriminant_analysis  import QuadraticDiscriminantAnalysis\n",
    "from   sklearn.model_selection        import GroupKFold\n",
    "from   sklearn.model_selection        import LeaveOneOut\n",
    "\n",
    "\n",
    "def classify(netmats, groups, classifier=None, print_folds=True, print_only_when_better_than_majority_acc = True):\n",
    "    \"\"\"Train a machine-learning classifier to differentiate groups based on\n",
    "    netmat edge strengths.\n",
    "\n",
    "    netmats:    (runs, edges) array containing per-subject netmats.\n",
    "    groups:     Number of subjects in each group\n",
    "    classifier: scikit-learn classifier object. The default is to use a\n",
    "                QuadraticDiscriminantAnalysis classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    if classifier is None:\n",
    "        classifier = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "\n",
    "    labels = np.zeros(netmats.shape[0], dtype=int)\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        start             = int(np.sum(groups[:i]))\n",
    "        end               = start + group\n",
    "        labels[start:end] = i\n",
    "\n",
    "    pipe = Pipeline([('preproc', StandardScaler()),\n",
    "                     ('fit',     classifier)])\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    predictions = np.zeros(labels.shape, dtype=int)\n",
    "    for fold, (train, test) in enumerate(loo.split(netmats)):\n",
    "\n",
    "        test_label   =  labels[test[0]]\n",
    "        train_labels = [labels[i] for i in train]\n",
    "\n",
    "        # Suppress this warning:\n",
    "        #   sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            pipe.fit(netmats[train], train_labels)\n",
    "            result = pipe.predict(netmats[test])[0]\n",
    "\n",
    "        predictions[fold] = result\n",
    "\n",
    "        if print_folds:\n",
    "            print(f'Training fold {fold+1:2d} label: {test_label}, prediction: {result}')\n",
    "\n",
    "    correct  = (labels == predictions).sum()\n",
    "    accuracy = correct / len(labels)\n",
    "    \n",
    "    majority_class_acc = max(np.bincount(labels)) / sum(np.bincount(labels))\n",
    "    if print_only_when_better_than_majority_acc and accuracy <= majority_class_acc:\n",
    "        return accuracy\n",
    "    print(f'-------------------------------------- >  Accuracy during cross-validation: {100 * accuracy:0.2f}% [majority class accuracy: {100 * majority_class_acc:0.2f}%]')\n",
    "    return accuracy\n",
    "    # predictions = pipe.predict(netmats)\n",
    "    # correct     = (labels == predictions).sum()\n",
    "    # accuracy    = correct / len(labels)\n",
    "    # print(f'Accuracy on input data:   {100 * accuracy:0.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_vars_if_interest = ['any_test_DTH_slips', 'any_combined_DTH_slips', 'any_App_slips']\n",
    "\n",
    "def prepare_for_classification(main_behav_data ,behav_var, n_ICs=50):\n",
    "\n",
    "    # get the data with subID and the behavior variable (and remove NaNs):\n",
    "    behav_data = main_behav_data[['subID', behav_var]].dropna().reset_index(drop=True)\n",
    "    # re-arange data to have all 1 in a row and all 0 in a row:\n",
    "    behav_data = behav_data.sort_values(by=[behav_var,'subID'], ascending=[True, True]).reset_index(drop=True)\n",
    "    n_0s = int((behav_data[behav_var] == 0).sum()) # Goal-directed\n",
    "    n_1s = int((behav_data[behav_var] == 1).sum()) # Habitual\n",
    "\n",
    "    # nruns = 1 if concatenate_sub_runs else 2\n",
    "    nruns = 1\n",
    "    ## Load as the relevant data as a ts object:\n",
    "    relevant_files = [f'{by_IDCH_sub_ID_ts_path}/dr_stage1_{subj}.txt' for subj in behav_data.subID]\n",
    "    ts = nets.load(relevant_files, 0.70, varnorm=0, nruns=nruns, thumbnaildir=f'{group_ICA_path}/groupICA{n_ICs}.sum')\n",
    "    Fnetmats = nets.netmats(ts, 'corr',   True)\n",
    "    Pnetmats = nets.netmats(ts, 'ridgep', True, 0.1) # partial correlations with regularization\n",
    "\n",
    "    return behav_data, ts, Fnetmats, Pnetmats, n_0s, n_1s\n",
    "\n",
    "\n",
    "def run_classification_for_behav_vars(behav_vars_of_interest, main_behav_data, classifier=None, n_ICs=50, print_folds=False):\n",
    "    print (f'\\n>>>>>>>>>>>>>>>>>> Classifier: {classifier} <<<<<<<<<<<<<<<<<<')\n",
    "    for behav_var in behav_vars_of_interest:\n",
    "        # behav_data, ts, Fnetmats, Pnetmats, n_0s, n_1s = prepare_for_classification(main_behav_data, behav_var, n_ICs=n_ICs)\n",
    "        behav_data, ts_completely_concat, Fnetmats_completely_concat, Pnetmats_completely_concat, n_0s, n_1s = prepare_for_classification(main_behav_data, behav_var, n_ICs=n_ICs)\n",
    "        print(f'---------------------------------   {behav_var}   ---------------------------------')\n",
    "        # print(f'>>> Partial corr (regular, leave one time series out, include all others, including the participants)')\n",
    "        # classify(Pnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\n",
    "        # print(f'>>> Full corr (regular, leave one time series out, include all others, including the participants)')\n",
    "        # classify(Fnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\n",
    "        print(f'>>> Partial corr (time series per subject completely concatenated)')\n",
    "        classify(Pnetmats_completely_concat, (n_0s, n_1s), classifier=classifier, print_folds=print_folds)\n",
    "        print(f'>>> Full corr (time series per subject completely concatenated)')\n",
    "        classify(Fnetmats_completely_concat, (n_0s, n_1s), classifier=classifier, print_folds=print_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['any_test_DTH_slips', 'any_combined_DTH_slips', 'any_App_slips']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav_vars_if_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>>>>>>>>>>>> Classifier: DecisionTreeClassifier() <<<<<<<<<<<<<<<<<<\n",
      "---------------------------------   any_test_DTH_slips   ---------------------------------\n",
      ">>> Partial corr (time series per subject completely concatenated)\n",
      ">>> Full corr (time series per subject completely concatenated)\n",
      "-------------------------------------- >  Accuracy during cross-validation: 80.95% [majority class accuracy: 57.14%]\n",
      "---------------------------------   any_combined_DTH_slips   ---------------------------------\n",
      ">>> Partial corr (time series per subject completely concatenated)\n",
      "-------------------------------------- >  Accuracy during cross-validation: 57.14% [majority class accuracy: 52.38%]\n",
      ">>> Full corr (time series per subject completely concatenated)\n",
      "---------------------------------   any_App_slips   ---------------------------------\n",
      ">>> Partial corr (time series per subject completely concatenated)\n",
      ">>> Full corr (time series per subject completely concatenated)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier # Multi layer perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=QuadraticDiscriminantAnalysis(), n_ICs=50, print_folds=False) # DEFAULT is QuadraticDiscriminantAnalysis, so without this argument we'll get the same result\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=LinearDiscriminantAnalysis(), n_ICs=50, print_folds=False) # DEFAULT is QuadraticDiscriminantAnalysis, so without this argument we'll get the same result\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=RandomForestClassifier(), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=SVC(), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=SVC(kernel='linear'), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=SVC(kernel='poly'), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=SVC(kernel='sigmoid'), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=LogisticRegression(), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=GradientBoostingClassifier(), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=KNeighborsClassifier(n_neighbors=3), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=GaussianNB(), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=BernoulliNB(), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=AdaBoostClassifier(n_estimators=50), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), n_ICs=50, print_folds=False)\n",
    "# run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=MLPClassifier(hidden_layer_sizes=(100,), max_iter=300), n_ICs=50, print_folds=False) # Neuroal network\n",
    "run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=DecisionTreeClassifier(), n_ICs=50, print_folds=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POWER !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(netmats, groups, classifier=None, print_folds=True, print_only_when_better_than_majority_acc = True, shuffleLabels=False):\n",
    "    labels = np.zeros(netmats.shape[0], dtype=int)\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        start             = int(np.sum(groups[:i]))\n",
    "        end               = start + group\n",
    "        labels[start:end] = i\n",
    "\n",
    "    if shuffleLabels:\n",
    "        np.random.shuffle(labels)\n",
    "        \n",
    "    pipe = Pipeline([('preproc', StandardScaler()),\n",
    "                     ('fit',     classifier)])\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    predictions = np.zeros(labels.shape, dtype=int)\n",
    "    for fold, (train, test) in enumerate(loo.split(netmats)):\n",
    "\n",
    "        test_label   =  labels[test[0]]\n",
    "        train_labels = [labels[i] for i in train]\n",
    "\n",
    "        # Suppress this warning:\n",
    "        #   sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            pipe.fit(netmats[train], train_labels)\n",
    "            result = pipe.predict(netmats[test])[0]\n",
    "\n",
    "        predictions[fold] = result\n",
    "\n",
    "        if print_folds:\n",
    "            print(f'Training fold {fold+1:2d} label: {test_label}, prediction: {result}')\n",
    "\n",
    "    correct  = (labels == predictions).sum()\n",
    "    accuracy = correct / len(labels)\n",
    "    \n",
    "    majority_class_acc = max(np.bincount(labels)) / sum(np.bincount(labels))\n",
    "    if (print_only_when_better_than_majority_acc and accuracy <= majority_class_acc) or shuffleLabels:\n",
    "        return accuracy\n",
    "    print(f'-------------------------------------- >  Accuracy during cross-validation: {100 * accuracy:0.2f}% [majority class accuracy: {100 * majority_class_acc:0.2f}%]')\n",
    "    return accuracy\n",
    "    # predictions = pipe.predict(netmats)\n",
    "    # correct     = (labels == predictions).sum()\n",
    "    # accuracy    = correct / len(labels)\n",
    "    # print(f'Accuracy on input data:   {100 * accuracy:0.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a null distibution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>>>>>>>>>>>> Classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      "---------------------------------   any_test_DTH_slips   ---------------------------------\n",
      ">>> Full corr (time series per subject completely concatenated)\n",
      "-------------------------------------- >  Accuracy during cross-validation: 80.95% [majority class accuracy: 57.14%]\n",
      ">>> Null distribution: 0\n",
      ">>> Null distribution: 10\n",
      ">>> Null distribution: 20\n",
      ">>> Null distribution: 30\n",
      ">>> Null distribution: 40\n",
      ">>> Null distribution: 50\n",
      ">>> Null distribution: 60\n",
      ">>> Null distribution: 70\n",
      ">>> Null distribution: 80\n",
      ">>> Null distribution: 90\n",
      ">>> Null distribution: 100\n",
      ">>> Null distribution: 110\n",
      ">>> Null distribution: 120\n",
      ">>> Null distribution: 130\n",
      ">>> Null distribution: 140\n",
      ">>> Null distribution: 150\n",
      ">>> Null distribution: 160\n",
      ">>> Null distribution: 170\n",
      ">>> Null distribution: 180\n",
      ">>> Null distribution: 190\n",
      ">>> Null distribution: 200\n",
      ">>> Null distribution: 210\n",
      ">>> Null distribution: 220\n",
      ">>> Null distribution: 230\n",
      ">>> Null distribution: 240\n",
      ">>> Null distribution: 250\n",
      ">>> Null distribution: 260\n",
      ">>> Null distribution: 270\n",
      ">>> Null distribution: 280\n",
      ">>> Null distribution: 290\n",
      ">>> Null distribution: 300\n",
      ">>> Null distribution: 310\n",
      ">>> Null distribution: 320\n",
      ">>> Null distribution: 330\n",
      ">>> Null distribution: 340\n",
      ">>> Null distribution: 350\n",
      ">>> Null distribution: 360\n",
      ">>> Null distribution: 370\n",
      ">>> Null distribution: 380\n",
      ">>> Null distribution: 390\n",
      ">>> Null distribution: 400\n",
      ">>> Null distribution: 410\n",
      ">>> Null distribution: 420\n",
      ">>> Null distribution: 430\n",
      ">>> Null distribution: 440\n",
      ">>> Null distribution: 450\n",
      ">>> Null distribution: 460\n",
      ">>> Null distribution: 470\n",
      ">>> Null distribution: 480\n",
      ">>> Null distribution: 490\n",
      ">>> Null distribution: 500\n",
      ">>> Null distribution: 510\n",
      ">>> Null distribution: 520\n",
      ">>> Null distribution: 530\n",
      ">>> Null distribution: 540\n",
      ">>> Null distribution: 550\n",
      ">>> Null distribution: 560\n",
      ">>> Null distribution: 570\n",
      ">>> Null distribution: 580\n",
      ">>> Null distribution: 590\n",
      ">>> Null distribution: 600\n",
      ">>> Null distribution: 610\n",
      ">>> Null distribution: 620\n",
      ">>> Null distribution: 630\n",
      ">>> Null distribution: 640\n",
      ">>> Null distribution: 650\n",
      ">>> Null distribution: 660\n",
      ">>> Null distribution: 670\n",
      ">>> Null distribution: 680\n",
      ">>> Null distribution: 690\n",
      ">>> Null distribution: 700\n",
      ">>> Null distribution: 710\n",
      ">>> Null distribution: 720\n",
      ">>> Null distribution: 730\n",
      ">>> Null distribution: 740\n",
      ">>> Null distribution: 750\n",
      ">>> Null distribution: 760\n",
      ">>> Null distribution: 770\n",
      ">>> Null distribution: 780\n",
      ">>> Null distribution: 790\n",
      ">>> Null distribution: 800\n",
      ">>> Null distribution: 810\n",
      ">>> Null distribution: 820\n",
      ">>> Null distribution: 830\n",
      ">>> Null distribution: 840\n",
      ">>> Null distribution: 850\n",
      ">>> Null distribution: 860\n",
      ">>> Null distribution: 870\n",
      ">>> Null distribution: 880\n",
      ">>> Null distribution: 890\n",
      ">>> Null distribution: 900\n",
      ">>> Null distribution: 910\n",
      ">>> Null distribution: 920\n",
      ">>> Null distribution: 930\n",
      ">>> Null distribution: 940\n",
      ">>> Null distribution: 950\n",
      ">>> Null distribution: 960\n",
      ">>> Null distribution: 970\n",
      ">>> Null distribution: 980\n",
      ">>> Null distribution: 990\n",
      ">>> Null distribution: 1000\n",
      ">>> Null distribution: 1010\n",
      ">>> Null distribution: 1020\n",
      ">>> Null distribution: 1030\n",
      ">>> Null distribution: 1040\n",
      ">>> Null distribution: 1050\n",
      ">>> Null distribution: 1060\n",
      ">>> Null distribution: 1070\n",
      ">>> Null distribution: 1080\n",
      ">>> Null distribution: 1090\n",
      ">>> Null distribution: 1100\n",
      ">>> Null distribution: 1110\n",
      ">>> Null distribution: 1120\n",
      ">>> Null distribution: 1130\n",
      ">>> Null distribution: 1140\n",
      ">>> Null distribution: 1150\n",
      ">>> Null distribution: 1160\n",
      ">>> Null distribution: 1170\n",
      ">>> Null distribution: 1180\n",
      ">>> Null distribution: 1190\n",
      ">>> Null distribution: 1200\n",
      ">>> Null distribution: 1210\n",
      ">>> Null distribution: 1220\n",
      ">>> Null distribution: 1230\n",
      ">>> Null distribution: 1240\n",
      ">>> Null distribution: 1250\n",
      ">>> Null distribution: 1260\n",
      ">>> Null distribution: 1270\n",
      ">>> Null distribution: 1280\n",
      ">>> Null distribution: 1290\n",
      ">>> Null distribution: 1300\n",
      ">>> Null distribution: 1310\n",
      ">>> Null distribution: 1320\n",
      ">>> Null distribution: 1330\n",
      ">>> Null distribution: 1340\n",
      ">>> Null distribution: 1350\n",
      ">>> Null distribution: 1360\n",
      ">>> Null distribution: 1370\n",
      ">>> Null distribution: 1380\n",
      ">>> Null distribution: 1390\n",
      ">>> Null distribution: 1400\n",
      ">>> Null distribution: 1410\n",
      ">>> Null distribution: 1420\n",
      ">>> Null distribution: 1430\n",
      ">>> Null distribution: 1440\n",
      ">>> Null distribution: 1450\n",
      ">>> Null distribution: 1460\n",
      ">>> Null distribution: 1470\n",
      ">>> Null distribution: 1480\n",
      ">>> Null distribution: 1490\n",
      ">>> Null distribution: 1500\n",
      ">>> Null distribution: 1510\n",
      ">>> Null distribution: 1520\n",
      ">>> Null distribution: 1530\n",
      ">>> Null distribution: 1540\n",
      ">>> Null distribution: 1550\n",
      ">>> Null distribution: 1560\n",
      ">>> Null distribution: 1570\n",
      ">>> Null distribution: 1580\n",
      ">>> Null distribution: 1590\n",
      ">>> Null distribution: 1600\n",
      ">>> Null distribution: 1610\n",
      ">>> Null distribution: 1620\n",
      ">>> Null distribution: 1630\n",
      ">>> Null distribution: 1640\n",
      ">>> Null distribution: 1650\n",
      ">>> Null distribution: 1660\n",
      ">>> Null distribution: 1670\n",
      ">>> Null distribution: 1680\n",
      ">>> Null distribution: 1690\n",
      ">>> Null distribution: 1700\n",
      ">>> Null distribution: 1710\n",
      ">>> Null distribution: 1720\n",
      ">>> Null distribution: 1730\n",
      ">>> Null distribution: 1740\n",
      ">>> Null distribution: 1750\n",
      ">>> Null distribution: 1760\n",
      ">>> Null distribution: 1770\n",
      ">>> Null distribution: 1780\n",
      ">>> Null distribution: 1790\n",
      ">>> Null distribution: 1800\n",
      ">>> Null distribution: 1810\n",
      ">>> Null distribution: 1820\n",
      ">>> Null distribution: 1830\n",
      ">>> Null distribution: 1840\n",
      ">>> Null distribution: 1850\n",
      ">>> Null distribution: 1860\n",
      ">>> Null distribution: 1870\n",
      ">>> Null distribution: 1880\n",
      ">>> Null distribution: 1890\n",
      ">>> Null distribution: 1900\n",
      ">>> Null distribution: 1910\n",
      ">>> Null distribution: 1920\n",
      ">>> Null distribution: 1930\n",
      ">>> Null distribution: 1940\n",
      ">>> Null distribution: 1950\n",
      ">>> Null distribution: 1960\n",
      ">>> Null distribution: 1970\n",
      ">>> Null distribution: 1980\n",
      ">>> Null distribution: 1990\n",
      ">>> Null distribution: 2000\n",
      ">>> Null distribution: 2010\n",
      ">>> Null distribution: 2020\n",
      ">>> Null distribution: 2030\n",
      ">>> Null distribution: 2040\n",
      ">>> Null distribution: 2050\n",
      ">>> Null distribution: 2060\n",
      ">>> Null distribution: 2070\n",
      ">>> Null distribution: 2080\n",
      ">>> Null distribution: 2090\n",
      ">>> Null distribution: 2100\n",
      ">>> Null distribution: 2110\n",
      ">>> Null distribution: 2120\n",
      ">>> Null distribution: 2130\n",
      ">>> Null distribution: 2140\n",
      ">>> Null distribution: 2150\n",
      ">>> Null distribution: 2160\n",
      ">>> Null distribution: 2170\n",
      ">>> Null distribution: 2180\n",
      ">>> Null distribution: 2190\n",
      ">>> Null distribution: 2200\n",
      ">>> Null distribution: 2210\n",
      ">>> Null distribution: 2220\n",
      ">>> Null distribution: 2230\n",
      ">>> Null distribution: 2240\n",
      ">>> Null distribution: 2250\n",
      ">>> Null distribution: 2260\n",
      ">>> Null distribution: 2270\n",
      ">>> Null distribution: 2280\n",
      ">>> Null distribution: 2290\n",
      ">>> Null distribution: 2300\n",
      ">>> Null distribution: 2310\n",
      ">>> Null distribution: 2320\n",
      ">>> Null distribution: 2330\n",
      ">>> Null distribution: 2340\n",
      ">>> Null distribution: 2350\n",
      ">>> Null distribution: 2360\n",
      ">>> Null distribution: 2370\n",
      ">>> Null distribution: 2380\n",
      ">>> Null distribution: 2390\n",
      ">>> Null distribution: 2400\n",
      ">>> Null distribution: 2410\n",
      ">>> Null distribution: 2420\n",
      ">>> Null distribution: 2430\n",
      ">>> Null distribution: 2440\n",
      ">>> Null distribution: 2450\n",
      ">>> Null distribution: 2460\n",
      ">>> Null distribution: 2470\n",
      ">>> Null distribution: 2480\n",
      ">>> Null distribution: 2490\n",
      ">>> Null distribution: 2500\n",
      ">>> Null distribution: 2510\n",
      ">>> Null distribution: 2520\n",
      ">>> Null distribution: 2530\n",
      ">>> Null distribution: 2540\n",
      ">>> Null distribution: 2550\n",
      ">>> Null distribution: 2560\n",
      ">>> Null distribution: 2570\n",
      ">>> Null distribution: 2580\n",
      ">>> Null distribution: 2590\n",
      ">>> Null distribution: 2600\n",
      ">>> Null distribution: 2610\n",
      ">>> Null distribution: 2620\n",
      ">>> Null distribution: 2630\n",
      ">>> Null distribution: 2640\n",
      ">>> Null distribution: 2650\n",
      ">>> Null distribution: 2660\n",
      ">>> Null distribution: 2670\n",
      ">>> Null distribution: 2680\n",
      ">>> Null distribution: 2690\n",
      ">>> Null distribution: 2700\n",
      ">>> Null distribution: 2710\n",
      ">>> Null distribution: 2720\n",
      ">>> Null distribution: 2730\n",
      ">>> Null distribution: 2740\n",
      ">>> Null distribution: 2750\n",
      ">>> Null distribution: 2760\n",
      ">>> Null distribution: 2770\n",
      ">>> Null distribution: 2780\n",
      ">>> Null distribution: 2790\n",
      ">>> Null distribution: 2800\n",
      ">>> Null distribution: 2810\n",
      ">>> Null distribution: 2820\n",
      ">>> Null distribution: 2830\n",
      ">>> Null distribution: 2840\n",
      ">>> Null distribution: 2850\n",
      ">>> Null distribution: 2860\n",
      ">>> Null distribution: 2870\n",
      ">>> Null distribution: 2880\n",
      ">>> Null distribution: 2890\n",
      ">>> Null distribution: 2900\n",
      ">>> Null distribution: 2910\n",
      ">>> Null distribution: 2920\n",
      ">>> Null distribution: 2930\n",
      ">>> Null distribution: 2940\n",
      ">>> Null distribution: 2950\n",
      ">>> Null distribution: 2960\n",
      ">>> Null distribution: 2970\n",
      ">>> Null distribution: 2980\n",
      ">>> Null distribution: 2990\n",
      ">>> Null distribution: 3000\n",
      ">>> Null distribution: 3010\n",
      ">>> Null distribution: 3020\n",
      ">>> Null distribution: 3030\n",
      ">>> Null distribution: 3040\n",
      ">>> Null distribution: 3050\n",
      ">>> Null distribution: 3060\n",
      ">>> Null distribution: 3070\n",
      ">>> Null distribution: 3080\n",
      ">>> Null distribution: 3090\n",
      ">>> Null distribution: 3100\n",
      ">>> Null distribution: 3110\n",
      ">>> Null distribution: 3120\n",
      ">>> Null distribution: 3130\n",
      ">>> Null distribution: 3140\n",
      ">>> Null distribution: 3150\n",
      ">>> Null distribution: 3160\n",
      ">>> Null distribution: 3170\n",
      ">>> Null distribution: 3180\n",
      ">>> Null distribution: 3190\n",
      ">>> Null distribution: 3200\n",
      ">>> Null distribution: 3210\n",
      ">>> Null distribution: 3220\n",
      ">>> Null distribution: 3230\n",
      ">>> Null distribution: 3240\n",
      ">>> Null distribution: 3250\n",
      ">>> Null distribution: 3260\n",
      ">>> Null distribution: 3270\n",
      ">>> Null distribution: 3280\n",
      ">>> Null distribution: 3290\n",
      ">>> Null distribution: 3300\n",
      ">>> Null distribution: 3310\n",
      ">>> Null distribution: 3320\n",
      ">>> Null distribution: 3330\n",
      ">>> Null distribution: 3340\n",
      ">>> Null distribution: 3350\n",
      ">>> Null distribution: 3360\n",
      ">>> Null distribution: 3370\n",
      ">>> Null distribution: 3380\n",
      ">>> Null distribution: 3390\n",
      ">>> Null distribution: 3400\n",
      ">>> Null distribution: 3410\n",
      ">>> Null distribution: 3420\n",
      ">>> Null distribution: 3430\n",
      ">>> Null distribution: 3440\n",
      ">>> Null distribution: 3450\n",
      ">>> Null distribution: 3460\n",
      ">>> Null distribution: 3470\n",
      ">>> Null distribution: 3480\n",
      ">>> Null distribution: 3490\n",
      ">>> Null distribution: 3500\n",
      ">>> Null distribution: 3510\n",
      ">>> Null distribution: 3520\n",
      ">>> Null distribution: 3530\n",
      ">>> Null distribution: 3540\n",
      ">>> Null distribution: 3550\n",
      ">>> Null distribution: 3560\n",
      ">>> Null distribution: 3570\n",
      ">>> Null distribution: 3580\n",
      ">>> Null distribution: 3590\n",
      ">>> Null distribution: 3600\n",
      ">>> Null distribution: 3610\n",
      ">>> Null distribution: 3620\n",
      ">>> Null distribution: 3630\n",
      ">>> Null distribution: 3640\n",
      ">>> Null distribution: 3650\n",
      ">>> Null distribution: 3660\n",
      ">>> Null distribution: 3670\n",
      ">>> Null distribution: 3680\n",
      ">>> Null distribution: 3690\n",
      ">>> Null distribution: 3700\n",
      ">>> Null distribution: 3710\n",
      ">>> Null distribution: 3720\n",
      ">>> Null distribution: 3730\n",
      ">>> Null distribution: 3740\n",
      ">>> Null distribution: 3750\n",
      ">>> Null distribution: 3760\n",
      ">>> Null distribution: 3770\n",
      ">>> Null distribution: 3780\n",
      ">>> Null distribution: 3790\n",
      ">>> Null distribution: 3800\n",
      ">>> Null distribution: 3810\n",
      ">>> Null distribution: 3820\n",
      ">>> Null distribution: 3830\n",
      ">>> Null distribution: 3840\n",
      ">>> Null distribution: 3850\n",
      ">>> Null distribution: 3860\n",
      ">>> Null distribution: 3870\n",
      ">>> Null distribution: 3880\n",
      ">>> Null distribution: 3890\n",
      ">>> Null distribution: 3900\n",
      ">>> Null distribution: 3910\n",
      ">>> Null distribution: 3920\n",
      ">>> Null distribution: 3930\n",
      ">>> Null distribution: 3940\n",
      ">>> Null distribution: 3950\n",
      ">>> Null distribution: 3960\n",
      ">>> Null distribution: 3970\n",
      ">>> Null distribution: 3980\n",
      ">>> Null distribution: 3990\n",
      ">>> Null distribution: 4000\n",
      ">>> Null distribution: 4010\n",
      ">>> Null distribution: 4020\n",
      ">>> Null distribution: 4030\n",
      ">>> Null distribution: 4040\n",
      ">>> Null distribution: 4050\n",
      ">>> Null distribution: 4060\n",
      ">>> Null distribution: 4070\n",
      ">>> Null distribution: 4080\n",
      ">>> Null distribution: 4090\n",
      ">>> Null distribution: 4100\n",
      ">>> Null distribution: 4110\n",
      ">>> Null distribution: 4120\n",
      ">>> Null distribution: 4130\n",
      ">>> Null distribution: 4140\n",
      ">>> Null distribution: 4150\n",
      ">>> Null distribution: 4160\n",
      ">>> Null distribution: 4170\n",
      ">>> Null distribution: 4180\n",
      ">>> Null distribution: 4190\n",
      ">>> Null distribution: 4200\n",
      ">>> Null distribution: 4210\n",
      ">>> Null distribution: 4220\n",
      ">>> Null distribution: 4230\n",
      ">>> Null distribution: 4240\n",
      ">>> Null distribution: 4250\n",
      ">>> Null distribution: 4260\n",
      ">>> Null distribution: 4270\n",
      ">>> Null distribution: 4280\n",
      ">>> Null distribution: 4290\n",
      ">>> Null distribution: 4300\n",
      ">>> Null distribution: 4310\n",
      ">>> Null distribution: 4320\n",
      ">>> Null distribution: 4330\n",
      ">>> Null distribution: 4340\n",
      ">>> Null distribution: 4350\n",
      ">>> Null distribution: 4360\n",
      ">>> Null distribution: 4370\n",
      ">>> Null distribution: 4380\n",
      ">>> Null distribution: 4390\n",
      ">>> Null distribution: 4400\n",
      ">>> Null distribution: 4410\n",
      ">>> Null distribution: 4420\n",
      ">>> Null distribution: 4430\n",
      ">>> Null distribution: 4440\n",
      ">>> Null distribution: 4450\n",
      ">>> Null distribution: 4460\n",
      ">>> Null distribution: 4470\n",
      ">>> Null distribution: 4480\n",
      ">>> Null distribution: 4490\n",
      ">>> Null distribution: 4500\n",
      ">>> Null distribution: 4510\n",
      ">>> Null distribution: 4520\n",
      ">>> Null distribution: 4530\n",
      ">>> Null distribution: 4540\n",
      ">>> Null distribution: 4550\n",
      ">>> Null distribution: 4560\n",
      ">>> Null distribution: 4570\n",
      ">>> Null distribution: 4580\n",
      ">>> Null distribution: 4590\n",
      ">>> Null distribution: 4600\n",
      ">>> Null distribution: 4610\n",
      ">>> Null distribution: 4620\n",
      ">>> Null distribution: 4630\n",
      ">>> Null distribution: 4640\n",
      ">>> Null distribution: 4650\n",
      ">>> Null distribution: 4660\n",
      ">>> Null distribution: 4670\n",
      ">>> Null distribution: 4680\n",
      ">>> Null distribution: 4690\n",
      ">>> Null distribution: 4700\n",
      ">>> Null distribution: 4710\n",
      ">>> Null distribution: 4720\n",
      ">>> Null distribution: 4730\n",
      ">>> Null distribution: 4740\n",
      ">>> Null distribution: 4750\n",
      ">>> Null distribution: 4760\n",
      ">>> Null distribution: 4770\n",
      ">>> Null distribution: 4780\n",
      ">>> Null distribution: 4790\n",
      ">>> Null distribution: 4800\n",
      ">>> Null distribution: 4810\n",
      ">>> Null distribution: 4820\n",
      ">>> Null distribution: 4830\n",
      ">>> Null distribution: 4840\n",
      ">>> Null distribution: 4850\n",
      ">>> Null distribution: 4860\n",
      ">>> Null distribution: 4870\n",
      ">>> Null distribution: 4880\n",
      ">>> Null distribution: 4890\n",
      ">>> Null distribution: 4900\n",
      ">>> Null distribution: 4910\n",
      ">>> Null distribution: 4920\n",
      ">>> Null distribution: 4930\n",
      ">>> Null distribution: 4940\n",
      ">>> Null distribution: 4950\n",
      ">>> Null distribution: 4960\n",
      ">>> Null distribution: 4970\n",
      ">>> Null distribution: 4980\n",
      ">>> Null distribution: 4990\n",
      "Accuracy: 0.8095238095238095\n",
      "Observed accuracy (95 perentile of null distribution): 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "behav_vars_if_interest = ['any_test_DTH_slips']\n",
    "\n",
    "def prepare_for_classification(main_behav_data ,behav_var, n_ICs=50):\n",
    "\n",
    "    # get the data with subID and the behavior variable (and remove NaNs):\n",
    "    behav_data = main_behav_data[['subID', behav_var]].dropna().reset_index(drop=True)\n",
    "    # re-arange data to have all 1 in a row and all 0 in a row:\n",
    "    behav_data = behav_data.sort_values(by=[behav_var,'subID'], ascending=[True, True]).reset_index(drop=True)\n",
    "    n_0s = int((behav_data[behav_var] == 0).sum()) # Goal-directed\n",
    "    n_1s = int((behav_data[behav_var] == 1).sum()) # Habitual\n",
    "\n",
    "    # nruns = 1 if concatenate_sub_runs else 2\n",
    "    nruns = 1\n",
    "    ## Load as the relevant data as a ts object:\n",
    "    relevant_files = [f'{by_IDCH_sub_ID_ts_path}/dr_stage1_{subj}.txt' for subj in behav_data.subID]\n",
    "    ts = nets.load(relevant_files, 0.70, varnorm=0, nruns=nruns, thumbnaildir=f'{group_ICA_path}/groupICA{n_ICs}.sum')\n",
    "    Fnetmats = nets.netmats(ts, 'corr',   True)\n",
    "    Pnetmats = nets.netmats(ts, 'ridgep', True, 0.1) # partial correlations with regularization\n",
    "\n",
    "    return behav_data, ts, Fnetmats, Pnetmats, n_0s, n_1s\n",
    "\n",
    "def run_classification_for_behav_vars(behav_vars_of_interest, main_behav_data, classifier=None, n_ICs=50, print_folds=False):\n",
    "    print (f'\\n>>>>>>>>>>>>>>>>>> Classifier: {classifier} <<<<<<<<<<<<<<<<<<')\n",
    "    for behav_var in behav_vars_of_interest:\n",
    "        # behav_data, ts, Fnetmats, Pnetmats, n_0s, n_1s = prepare_for_classification(main_behav_data, behav_var, n_ICs=n_ICs)\n",
    "        behav_data, ts_completely_concat, Fnetmats_completely_concat, Pnetmats_completely_concat, n_0s, n_1s = prepare_for_classification(main_behav_data, behav_var, n_ICs=n_ICs)\n",
    "        print(f'---------------------------------   {behav_var}   ---------------------------------')\n",
    "        # print(f'>>> Partial corr (regular, leave one time series out, include all others, including the participants)')\n",
    "        # classify(Pnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\n",
    "        # print(f'>>> Full corr (regular, leave one time series out, include all others, including the participants)')\n",
    "        # classify(Fnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\n",
    "        # print(f'>>> Partial corr (time series per subject completely concatenated)')\n",
    "        # accuracy = classify(Pnetmats_completely_concat, (n_0s, n_1s), classifier=classifier, print_folds=print_folds)\n",
    "        # accuracies.append(float(accuracy))\n",
    "        print(f'>>> Full corr (time series per subject completely concatenated)')\n",
    "        accuracy = classify(Fnetmats_completely_concat, (n_0s, n_1s), classifier=classifier, print_folds=print_folds)\n",
    "        accuracy = float(accuracy)\n",
    "\n",
    "    null_distribuiton = []\n",
    "    for i in range(5000):\n",
    "        if i % 10 == 0:\n",
    "            print(f'>>> Null distribution: {i}')\n",
    "        null_accuracy = classify(Fnetmats_completely_concat, (n_0s, n_1s), classifier=classifier, print_folds=False, shuffleLabels=True)\n",
    "        null_distribuiton.append(float(null_accuracy))\n",
    "\n",
    "    return accuracy, null_distribuiton\n",
    "\n",
    "accuracy, null_distribuiton = run_classification_for_behav_vars(behav_vars_if_interest, main_behav_data, classifier=DecisionTreeClassifier(random_state=123), n_ICs=50, print_folds=False)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "observed_accuracy_threshold = float(np.percentile(null_distribuiton, 95))\n",
    "print(f'Observed accuracy (95 perentile of null distribution): {observed_accuracy_threshold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the null distribution:\n",
    "plt.hist(null_distribuiton, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 10 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 13 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 16 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 19 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 22 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 25 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 28 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 31 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 34 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 37 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 40 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 43 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 46 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 49 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n",
      ">>> Simulation: 720\n",
      ">>> Simulation: 730\n",
      ">>> Simulation: 740\n",
      ">>> Simulation: 750\n",
      ">>> Simulation: 760\n",
      ">>> Simulation: 770\n",
      ">>> Simulation: 780\n",
      ">>> Simulation: 790\n",
      ">>> Simulation: 800\n",
      ">>> Simulation: 810\n",
      ">>> Simulation: 820\n",
      ">>> Simulation: 830\n",
      ">>> Simulation: 840\n",
      ">>> Simulation: 850\n",
      ">>> Simulation: 860\n",
      ">>> Simulation: 870\n",
      ">>> Simulation: 880\n",
      ">>> Simulation: 890\n",
      ">>> Simulation: 900\n",
      ">>> Simulation: 910\n",
      ">>> Simulation: 920\n",
      ">>> Simulation: 930\n",
      ">>> Simulation: 940\n",
      ">>> Simulation: 950\n",
      ">>> Simulation: 960\n",
      ">>> Simulation: 970\n",
      ">>> Simulation: 980\n",
      ">>> Simulation: 990\n",
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 52 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n",
      ">>> Simulation: 460\n",
      ">>> Simulation: 470\n",
      ">>> Simulation: 480\n",
      ">>> Simulation: 490\n",
      ">>> Simulation: 500\n",
      ">>> Simulation: 510\n",
      ">>> Simulation: 520\n",
      ">>> Simulation: 530\n",
      ">>> Simulation: 540\n",
      ">>> Simulation: 550\n",
      ">>> Simulation: 560\n",
      ">>> Simulation: 570\n",
      ">>> Simulation: 580\n",
      ">>> Simulation: 590\n",
      ">>> Simulation: 600\n",
      ">>> Simulation: 610\n",
      ">>> Simulation: 620\n",
      ">>> Simulation: 630\n",
      ">>> Simulation: 640\n",
      ">>> Simulation: 650\n",
      ">>> Simulation: 660\n",
      ">>> Simulation: 670\n",
      ">>> Simulation: 680\n",
      ">>> Simulation: 690\n",
      ">>> Simulation: 700\n",
      ">>> Simulation: 710\n"
     ]
    }
   ],
   "source": [
    "def bootstrapping(netmats, groups, classifier=None, sample_size=10):\n",
    "    labels = np.zeros(netmats.shape[0], dtype=int)\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        start             = int(np.sum(groups[:i]))\n",
    "        end               = start + group\n",
    "        labels[start:end] = i\n",
    "\n",
    "    # sample with replacement\n",
    "    sample_indices = np.random.choice(netmats.shape[0], size=(sample_size), replace=True)\n",
    "    # now accordingly do the sampling from the netmats and labels:\n",
    "    sample_netmats = netmats[sample_indices]\n",
    "    sample_labels = labels[sample_indices]\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline([('preproc', StandardScaler()),\n",
    "                     ('fit',     classifier)])\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    predictions = np.zeros(sample_labels.shape, dtype=int)    \n",
    "\n",
    "    for fold, (train, test) in enumerate(loo.split(sample_netmats)):\n",
    "\n",
    "        test_label   = sample_labels[test[0]]\n",
    "        train_labels = [sample_labels[i] for i in train]\n",
    "\n",
    "        # Suppress this warning:\n",
    "        #   sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            pipe.fit(sample_netmats[train], train_labels)\n",
    "            result = pipe.predict(sample_netmats[test])[0]\n",
    "\n",
    "        predictions[fold] = result\n",
    "\n",
    "\n",
    "    correct  = (sample_labels == predictions).sum()\n",
    "    accuracy = correct / len(sample_labels)\n",
    "    \n",
    "    return float(accuracy)\n",
    "    # predictions = pipe.predict(netmats)\n",
    "\n",
    "def run_many_bootstraps(netmats, groups, observed_accuracy_threshold, classifier=None, sample_size=10, n_simulations=1000):\n",
    "    print (f'\\n>>>>>>>>>>>>>>>>>> Bootstrapping for sample size: {sample_size} with classifier: {classifier} <<<<<<<<<<<<<<<<<<')\n",
    "    accuracies = []\n",
    "    for i in range(n_simulations):\n",
    "        if i % 10 == 0:\n",
    "            print(f'>>> Simulation: {i}')\n",
    "        accuracy = bootstrapping(netmats, groups, classifier=classifier, sample_size=sample_size)\n",
    "        accuracies.append(float(accuracy))\n",
    "    power = sum([1 for acc in accuracies if acc > observed_accuracy_threshold]) / n_simulations\n",
    "    return power\n",
    "\n",
    "behav_var = 'any_test_DTH_slips'\n",
    "behav_data, ts_completely_concat, Fnetmats_completely_concat, Pnetmats_completely_concat, n_0s, n_1s = prepare_for_classification(main_behav_data, behav_var, n_ICs=50)\n",
    "# power = run_many_bootstraps(Fnetmats_completely_concat, (n_0s, n_1s), observed_accuracy_threshold, classifier=DecisionTreeClassifier(random_state=123), sample_size=40, n_simulations=100)\n",
    "\n",
    "sim_sample_size_range = range(10, 53, 3)\n",
    "Ns = [n for n in sim_sample_size_range]\n",
    "Powers = [run_many_bootstraps(Fnetmats_completely_concat, (n_0s, n_1s), observed_accuracy_threshold, classifier=DecisionTreeClassifier(random_state=123), sample_size=sim_sample_size, n_simulations=1000) for sim_sample_size in sim_sample_size_range]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle the results (null distribution, the observed accuracy, observed accuracy threshold, sim_sample_size_range, Ns, Powers):\n",
    "import pickle\n",
    "with open(f'./Power_analysis_{behav_var}_DecisionTreeClassifier.pkl', 'wb') as f:\n",
    "    pickle.dump([null_distribuiton, accuracy, observed_accuracy_threshold, sim_sample_size_range, Ns, Powers], f)\n",
    "\n",
    "# # Load the pickle file:\n",
    "# with open(f'{whole_network_path}/Power_analysis_{behav_var}_DecisionTreeClassifier.pkl', 'rb') as f:\n",
    "#     null_distribuiton, accuracy, observed_accuracy_threshold, sim_sample_size_range, Ns, Powers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_sample_size_range = range(10, 53, 3)\n",
    "Ns = [n for n in sim_sample_size_range]\n",
    "Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>>>>>>>>>>>> Bootstrapping for sample size: 20 with classifier: DecisionTreeClassifier(random_state=123) <<<<<<<<<<<<<<<<<<\n",
      ">>> Simulation: 0\n",
      ">>> Simulation: 10\n",
      ">>> Simulation: 20\n",
      ">>> Simulation: 30\n",
      ">>> Simulation: 40\n",
      ">>> Simulation: 50\n",
      ">>> Simulation: 60\n",
      ">>> Simulation: 70\n",
      ">>> Simulation: 80\n",
      ">>> Simulation: 90\n",
      ">>> Simulation: 100\n",
      ">>> Simulation: 110\n",
      ">>> Simulation: 120\n",
      ">>> Simulation: 130\n",
      ">>> Simulation: 140\n",
      ">>> Simulation: 150\n",
      ">>> Simulation: 160\n",
      ">>> Simulation: 170\n",
      ">>> Simulation: 180\n",
      ">>> Simulation: 190\n",
      ">>> Simulation: 200\n",
      ">>> Simulation: 210\n",
      ">>> Simulation: 220\n",
      ">>> Simulation: 230\n",
      ">>> Simulation: 240\n",
      ">>> Simulation: 250\n",
      ">>> Simulation: 260\n",
      ">>> Simulation: 270\n",
      ">>> Simulation: 280\n",
      ">>> Simulation: 290\n",
      ">>> Simulation: 300\n",
      ">>> Simulation: 310\n",
      ">>> Simulation: 320\n",
      ">>> Simulation: 330\n",
      ">>> Simulation: 340\n",
      ">>> Simulation: 350\n",
      ">>> Simulation: 360\n",
      ">>> Simulation: 370\n",
      ">>> Simulation: 380\n",
      ">>> Simulation: 390\n",
      ">>> Simulation: 400\n",
      ">>> Simulation: 410\n",
      ">>> Simulation: 420\n",
      ">>> Simulation: 430\n",
      ">>> Simulation: 440\n",
      ">>> Simulation: 450\n"
     ]
    }
   ],
   "source": [
    "run_many_bootstraps(Fnetmats_completely_concat, (n_0s, n_1s), observed_accuracy_threshold, classifier=DecisionTreeClassifier(random_state=123), sample_size=20, n_simulations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "assigning None to unbound local 'subj'\n",
      "assigning None to unbound local 'subj'\n"
     ]
    }
   ],
   "source": [
    "behav_vars_if_interest = ['any_test_DTH_slips']\n",
    "\n",
    "\n",
    "# get the data with subID and the behavior variable (and remove NaNs):\n",
    "behav_data = main_behav_data[['subID', behav_var]].dropna().reset_index(drop=True)\n",
    "# re-arange data to have all 1 in a row and all 0 in a row:\n",
    "behav_data = behav_data.sort_values(by=[behav_var,'subID'], ascending=[True, True]).reset_index(drop=True)\n",
    "n_0s = int((behav_data[behav_var] == 0).sum()) # Goal-directed\n",
    "n_1s = int((behav_data[behav_var] == 1).sum()) # Habitual\n",
    "\n",
    "# nruns = 1 if concatenate_sub_runs else 2\n",
    "nruns = 1\n",
    "## Load as the relevant data as a ts object:\n",
    "relevant_files = [f'{by_IDCH_sub_ID_ts_path}/dr_stage1_{subj}.txt' for subj in behav_data.subID]\n",
    "ts = nets.load(relevant_files, 0.70, varnorm=0, nruns=nruns, thumbnaildir=f'{group_ICA_path}/groupICA{n_ICs}.sum')\n",
    "Fnetmats = nets.netmats(ts, 'corr',   True)\n",
    "# sample with replacement n times:\n",
    "n = 1000\n",
    "n_samples = 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fsl.nets.load.TimeSeries at 0x7fb6290b19a0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_102.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_103.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_104.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_107.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_108.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_109.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_111.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_112.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_113.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_117.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_121.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_122.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_101.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_105.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_106.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_110.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_114.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_115.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_118.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_119.txt',\n",
       " '/export/home/ranigera/IDCH-rsfMRI/data/whole_networks/IC_ts_by_IDCH_sub_ID/dr_stage1_120.txt']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(netmats, groups, classifier=None, print_folds=True, print_only_when_better_than_majority_acc = True, shuffleLabels=False):\n",
    "    labels = np.zeros(netmats.shape[0], dtype=int)\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        start             = int(np.sum(groups[:i]))\n",
    "        end               = start + group\n",
    "        labels[start:end] = i\n",
    "\n",
    "    if shuffleLabels:\n",
    "        np.random.shuffle(labels)\n",
    "        \n",
    "    pipe = Pipeline([('preproc', StandardScaler()),\n",
    "                     ('fit',     classifier)])\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    predictions = np.zeros(labels.shape, dtype=int)\n",
    "    for fold, (train, test) in enumerate(loo.split(netmats)):\n",
    "\n",
    "        test_label   =  labels[test[0]]\n",
    "        train_labels = [labels[i] for i in train]\n",
    "\n",
    "        # Suppress this warning:\n",
    "        #   sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            pipe.fit(netmats[train], train_labels)\n",
    "            result = pipe.predict(netmats[test])[0]\n",
    "\n",
    "        predictions[fold] = result\n",
    "\n",
    "        if print_folds:\n",
    "            print(f'Training fold {fold+1:2d} label: {test_label}, prediction: {result}')\n",
    "\n",
    "    correct  = (labels == predictions).sum()\n",
    "    accuracy = correct / len(labels)\n",
    "    \n",
    "    majority_class_acc = max(np.bincount(labels)) / sum(np.bincount(labels))\n",
    "    if print_only_when_better_than_majority_acc and accuracy <= majority_class_acc:\n",
    "        return accuracy\n",
    "    print(f'-------------------------------------- >  Accuracy during cross-validation: {100 * accuracy:0.2f}% [majority class accuracy: {100 * majority_class_acc:0.2f}%]')\n",
    "    return accuracy\n",
    "    # predictions = pipe.predict(netmats)\n",
    "    # correct     = (labels == predictions).sum()\n",
    "    # accuracy    = correct / len(labels)\n",
    "    # print(f'Accuracy on input data:   {100 * accuracy:0.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>>>>>>>>>>>> Classifier: DecisionTreeClassifier() <<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>> Classifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDecisionTreeClassifier()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m <<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m behav_var \u001b[38;5;129;01min\u001b[39;00m behav_vars_of_interest:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# behav_data, ts, Fnetmats, Pnetmats, n_0s, n_1s = prepare_for_classification(main_behav_data, behav_var, n_ICs=n_ICs)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     behav_data, ts_completely_concat, Fnetmats_completely_concat, Pnetmats_completely_concat, n_0s, n_1s \u001b[38;5;241m=\u001b[39m prepare_for_permutation_test(main_behav_data, behav_var, n_ICs\u001b[38;5;241m=\u001b[39mn_ICs)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------------------   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbehav_var\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   ---------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# print(f'>>> Partial corr (regular, leave one time series out, include all others, including the participants)')\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# classify(Pnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# print(f'>>> Full corr (regular, leave one time series out, include all others, including the participants)')\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# classify(Fnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# print(f'>>> Partial corr (time series per subject completely concatenated)')\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# classify(Pnetmats_completely_concat, (n_0s, n_1s), classifier=DecisionTreeClassifier(), print_folds=False)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 19\u001b[0m, in \u001b[0;36mprepare_for_permutation_test\u001b[0;34m(main_behav_data, behav_var, n_ICs)\u001b[0m\n\u001b[1;32m     17\u001b[0m ts \u001b[38;5;241m=\u001b[39m nets\u001b[38;5;241m.\u001b[39mload(relevant_files, \u001b[38;5;241m0.70\u001b[39m, varnorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, nruns\u001b[38;5;241m=\u001b[39mnruns, thumbnaildir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_ICA_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/groupICA\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_ICs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.sum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m Fnetmats \u001b[38;5;241m=\u001b[39m nets\u001b[38;5;241m.\u001b[39mnetmats(ts, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorr\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m Pnetmats \u001b[38;5;241m=\u001b[39m nets\u001b[38;5;241m.\u001b[39mnetmats(ts, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridgep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;66;03m# partial correlations with regularization\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m behav_data, ts, Fnetmats, Pnetmats, n_0s, n_1s\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:1197\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_line:\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mpydev_original_step_cmd)\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_return:  \u001b[38;5;66;03m# return event\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m     back \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "behav_vars_of_interest = ['any_test_DTH_slips']\n",
    "print (f'\\n>>>>>>>>>>>>>>>>>> Classifier: {DecisionTreeClassifier()} <<<<<<<<<<<<<<<<<<')\n",
    "for behav_var in behav_vars_of_interest:\n",
    "    # behav_data, ts, Fnetmats, Pnetmats, n_0s, n_1s = prepare_for_classification(main_behav_data, behav_var, n_ICs=n_ICs)\n",
    "    behav_data, ts_completely_concat, Fnetmats_completely_concat, Pnetmats_completely_concat, n_0s, n_1s = prepare_for_permutation_test(main_behav_data, behav_var, n_ICs=n_ICs)\n",
    "    print(f'---------------------------------   {behav_var}   ---------------------------------')\n",
    "    # print(f'>>> Partial corr (regular, leave one time series out, include all others, including the participants)')\n",
    "    # classify(Pnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\n",
    "    # print(f'>>> Full corr (regular, leave one time series out, include all others, including the participants)')\n",
    "    # classify(Fnetmats, (n_0s*2, n_1s*2), classifier=classifier, print_folds=print_folds)\n",
    "    # print(f'>>> Partial corr (time series per subject completely concatenated)')\n",
    "    # classify(Pnetmats_completely_concat, (n_0s, n_1s), classifier=DecisionTreeClassifier(), print_folds=False)\n",
    "    print(f'>>> Full corr (time series per subject completely concatenated)')\n",
    "    classify(Fnetmats_completely_concat, (n_0s, n_1s), classifier=DecisionTreeClassifier(), print_folds=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next:\n",
    "- Decide on parcellatino and add dimensionality reduction\n",
    "\n",
    "For the univariate analysis:\n",
    "- maybe also use less components.\n",
    "- mayb use only hypothesized components/regions.\n",
    "- Maybe clean the data and throw away bad components (clean up) [many of the last ones afre so good]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
